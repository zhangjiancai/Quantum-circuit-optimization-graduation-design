{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class PPO:\n",
    "    \"\"\"Proximal Policy Optimization (PPO) Algorithm.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, lr=3e-4, gamma=0.99, clip_epsilon=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the PPO algorithm.\n",
    "\n",
    "        :param agent: The reinforcement learning agent model (e.g., neural network)\n",
    "        :param lr: Learning rate for the optimizer\n",
    "        :param gamma: Discount factor for future rewards\n",
    "        :param clip_epsilon: Clipping parameter to limit policy updates in PPO\n",
    "        \"\"\"\n",
    "        self.agent = agent\n",
    "        self.optimizer = torch.optim.Adam(agent.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "    def update(self, states, actions, rewards, dones, old_log_probs, values):\n",
    "        \"\"\"Update PPO agent based on collected experiences.\"\"\"\n",
    "        returns, advantages = self.compute_gae(rewards, dones, values)\n",
    "        new_policy, new_values = self.agent(states)\n",
    "        new_log_probs = self.compute_log_probs(new_policy, actions)\n",
    "\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)  # Normalize advantages\n",
    "        \n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # Value function loss clipped by a prediction error (optional, not in original message but common practice)\n",
    "        value_clipped = values + torch.clamp(new_values - values, -self.clip_epsilon, self.clip_epsilon)\n",
    "        value_losses_clipped = (new_values - returns).pow(2)\n",
    "        value_losses_original = (value_clipped - returns).pow(2)\n",
    "        value_loss = torch.max(value_losses_original, value_losses_clipped).mean()\n",
    "\n",
    "        entropy = Categorical(new_policy).entropy().mean()  # Encourage exploration\n",
    "\n",
    "        # Total loss with entropy regularization\n",
    "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy  # Assuming a coefficient for entropy of 0.01\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # Optionally return training statistics\n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'loss': total_loss.item(),\n",
    "            'avg_ratio': ratio.mean().item()\n",
    "            }\n",
    "\n",
    "    def compute_gae(self, rewards, dones, values):\n",
    "        \"\"\"Compute Generalized Advantage Estimation (GAE) for each step.\"\"\"\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        gae_lambda = 0.95  # GAE's lambda parameter\n",
    "        next_advantage = 0\n",
    "\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "            next_advantage = delta + self.gamma * gae_lambda * next_advantage * (1 - dones[t])\n",
    "            advantages[t] = next_advantage\n",
    "        \n",
    "        returns = advantages + values\n",
    "        return returns, advantages\n",
    "\n",
    "    def compute_log_probs(self, policy_dist, actions):\n",
    "        \"\"\"\n",
    "        计算所选动作的对数概率。\n",
    "        确保在传递给Categorical分布前进行规范化。\n",
    "        \"\"\"\n",
    "        # 指数化以获得未规范化的概率\n",
    "        unnormalized_probs = torch.exp(policy_dist)\n",
    "    \n",
    "        # 规范化以满足simplex约束\n",
    "        probs = unnormalized_probs / unnormalized_probs.sum(dim=-1, keepdim=True)\n",
    "    s\n",
    "        # 确保没有NaN或无穷大等数值问题\n",
    "        probs[torch.isnan(probs)] = 0.0  # 将NaN置为零处理\n",
    "        probs[probs == float('inf')] = 0.0  # 类似地处理无穷大\n",
    "    \n",
    "        # 现在创建Categorical分布并计算对数概率\n",
    "        dist = Categorical(probs=probs)\n",
    "        return dist.log_prob(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 假设的Agent模型简化为一个简单的神经网络结构\n",
    "class TestAgent(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestAgent, self).__init__()\n",
    "        self.fc = torch.nn.Linear(10, 2)  # 简单的全连接层，输出动作概率分布和值函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        policy_logits = x[:, :1]  # 动作概率分布的对数\n",
    "        values = x[:, 1:]  # 状态值函数\n",
    "        return policy_logits, values.squeeze()\n",
    "\n",
    "# 实例化PPO类和测试Agent\n",
    "ppo_agent = PPO(TestAgent(), lr=1e-3, gamma=0.99, clip_epsilon=0.2)\n",
    "\n",
    "# 生成模拟数据\n",
    "num_samples = 32  # 假设的样本数量\n",
    "state_dim = 10  # 状态空间维度\n",
    "action_dim = 1  # 动作空间维度，简化为二元动作问题\n",
    "\n",
    "states = torch.randn(num_samples, state_dim)  # 随机状态数据\n",
    "actions = torch.randint(0, 2, (num_samples, action_dim)).float()  # 随机动作数据\n",
    "rewards = torch.randn(num_samples, 1)  # 随机奖励数据\n",
    "dones = torch.zeros(num_samples, 1, dtype=torch.bool)  # 完成标志，假设所有序列未完成\n",
    "# 对于最后一条序列设置done为True，模拟序列结束\n",
    "dones[-1] = True\n",
    "values = ppo_agent.agent(states)[1]  # 初始值函数预测\n",
    "old_log_probs = ppo_agent.compute_log_probs(ppo_agent.agent(states)[0], actions)\n",
    "\n",
    "# 调用update方法并打印训练统计信息\n",
    "training_stats = ppo_agent.update(states, actions, rewards, dones, old_log_probs, values)\n",
    "print(\"Training Statistics:\")\n",
    "for key, value in training_stats.items():\n",
    "    print(f\"{key}: {value.item()}\")\n",
    "\n",
    "# 注意：此处的compute_gae和compute_log_probs方法需要在PPO类中正确实现，以匹配测试数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义PPO类，实现近端策略优化算法\n",
    "\n",
    "class PPO:\n",
    "    \"\"\"\n",
    "    近端策略优化（Proximal Policy Optimization，PPO）算法。\n",
    "\n",
    "    一种强化学习算法，通过信任区域方法进行策略更新，平衡探索与利用。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent, lr=3e-4, gamma=0.99, clip_epsilon=0.2):\n",
    "        \"\"\"\n",
    "        初始化PPO算法。\n",
    "\n",
    "        参数:\n",
    "        - agent: 强化学习智能体模型，通常为神经网络\n",
    "        - lr: 优化器的学习率\n",
    "        - gamma: 未来奖励的折现因子\n",
    "        - clip_epsilon: PPO中用于限制策略更新的裁剪参数\n",
    "        \"\"\"\n",
    "        self.agent = agent  # 设置智能体模型\n",
    "        self.optimizer = torch.optim.Adam(agent.parameters(), lr=lr)  # 使用Adam优化器调整模型参数\n",
    "        self.gamma = gamma  # 折现因子，用于计算未来奖励的当前价值\n",
    "        self.clip_epsilon = clip_epsilon  # 策略更新时的裁剪范围界限\n",
    "\n",
    "    def update(self, states, actions, rewards, dones, old_log_probs, values):\n",
    "        \"\"\"\n",
    "        根据收集的经验数据更新PPO智能体。\n",
    "\n",
    "        步骤包括计算广义优势估计（GAE）、新旧策略的比率、策略损失、价值损失以及加入熵正则化项的总损失，\n",
    "        并执行反向传播和参数更新。\n",
    "        \"\"\"\n",
    "        # 计算广义优势估计（GAE）和回报\n",
    "        returns, advantages = self.compute_gae(rewards, dones, values)\n",
    "\n",
    "        # 从当前策略中获取新的动作分布和状态价值\n",
    "        new_policy, new_values = self.agent(states)\n",
    "\n",
    "        # 计算新策略下动作的对数概率\n",
    "        new_log_probs = self.compute_log_probs(new_policy, actions)\n",
    "\n",
    "        # 计算重要性采样比值（ratio）\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "        # 对优势进行标准化处理，以稳定训练\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # 计算损失函数的两部分：策略损失（通过裁剪）和价值函数损失（可选的裁剪形式）\n",
    "        # 假设surr1和surr2的计算方式\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # 价值函数损失计算，考虑了裁剪以减少更新波动\n",
    "        # 假设returns的原始形状是[batch_size, sequence_length]\n",
    "        returns = returns.sum(dim=1, keepdim=True)  # 累加sequence_length维度，添加keepdim保持二维结构\n",
    "        # 假设new_values的原始形状是[batch_size, sequence_length]\n",
    "        new_values_aggregated = new_values.sum(dim=1, keepdim=True)  # 累加sequence_length维度，保持二维结构\n",
    "        value_loss = F.mse_loss(new_values_aggregated, returns)\n",
    "\n",
    "        # 计算策略熵，鼓励探索\n",
    "        new_policy_normalized = F.softmax(new_policy, dim=-1)\n",
    "        # 现在使用归一化后的概率分布来计算熵\n",
    "        entropy = Categorical(probs=new_policy_normalized).entropy().mean()\n",
    "\n",
    "        # 总损失，结合策略损失、价值损失和熵项\n",
    "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        # 执行优化步骤：梯度清零、反向传播、参数更新\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 返回训练统计信息，供监控和调试\n",
    "        return {\n",
    "            '策略损失': policy_loss.item(),\n",
    "            '价值损失': value_loss.item(),\n",
    "            '熵': entropy.item(),\n",
    "            '总损失': total_loss.item(),\n",
    "            '平均比率': ratio.mean().item()\n",
    "        }\n",
    "\n",
    "    def compute_gae(self, rewards, dones, values):\n",
    "        \"\"\"\n",
    "        计算每个时间步的广义优势估计（Generalized Advantage Estimation, GAE）。\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        gae_lambda = 0.95  # GAE中的衰减因子λ\n",
    "        last_gae_lam = 0\n",
    "\n",
    "        # 修改循环，使其不包括最后一个元素，避免索引越界\n",
    "        for t in reversed(range(len(rewards) - 1)):  # 减1以避免索引越界\n",
    "            next_non_terminal = 1.0 - dones[t+1]\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * next_non_terminal - values[t]\n",
    "            advantages[t] = delta + self.gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            last_gae_lam = advantages[t]\n",
    "\n",
    "        # 对于序列的第一个元素（t=0），我们需要特殊处理，这里直接设置为累计的优势\n",
    "        # 注意，因为我们已经处理到了t=1（原序列的倒数第二个元素），所以不需要额外的循环迭代\n",
    "        #advantages[0] = advantages[0]  # 这一行实际上不需要操作，因为第一个元素的advantage已经在循环中计算好\n",
    "\n",
    "        returns = advantages + values\n",
    "        return returns, advantages\n",
    "\n",
    "    def compute_log_probs(self, policy_dist, actions):\n",
    "        \"\"\"\n",
    "        计算在给定动作分布下选择的动作的对数概率。\n",
    "\n",
    "        确保在传递给Categorical分布之前进行规范化。\n",
    "        \"\"\"\n",
    "        # 计算未规范化概率\n",
    "        unnormalized_probs = torch.exp(policy_dist)\n",
    "\n",
    "        # 规范化概率以满足概率分布的约束\n",
    "        probs = unnormalized_probs / unnormalized_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # 处理可能出现的NaN或无穷大值\n",
    "        # 将它们设为0以避免数值问题\n",
    "        probs[torch.isnan(probs)] = 0.0\n",
    "        probs[probs == float('inf')] = 0.0\n",
    "\n",
    "        # 创建Categorical分布对象并计算对数概率\n",
    "        dist = Categorical(probs=probs)\n",
    "        log_probs = dist.log_prob(actions)  # 计算对数概率\n",
    "\n",
    "        return log_probs\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Statistics: {'策略损失': 4.0435789827597546e-08, '价值损失': 10999.3134765625, '熵': 0.6894959211349487, '总损失': 5499.64990234375, '平均比率': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# 构建一个简单的智能体模型作为示例\n",
    "class SimpleActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(SimpleActorCritic, self).__init__()\n",
    "        self.shared_layers = nn.Linear(state_dim, 128)\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = torch.relu(self.shared_layers(x))\n",
    "        policy_logits = self.policy_head(shared_out)\n",
    "        state_values = self.value_head(shared_out)\n",
    "        return policy_logits, state_values.unsqueeze(-1)\n",
    "\n",
    "# 生成模拟数据\n",
    "def generate_fake_data(agent, num_steps, state_dim, action_dim):\n",
    "    states = torch.randn(num_steps, state_dim)\n",
    "    actions = Categorical(logits=agent(states)[0]).sample()\n",
    "    rewards = torch.randn(num_steps, 1)\n",
    "    dones = torch.zeros(num_steps, 1, dtype=torch.float)\n",
    "    dones[-1] = 1  # 最后一步设置为终止状态\n",
    "    values = agent(states)[1]\n",
    "    old_log_probs = Categorical(logits=agent(states)[0]).log_prob(actions)\n",
    "    return states, actions, rewards, dones, old_log_probs, values\n",
    "\n",
    "# 实例化PPO类和智能体模型\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = SimpleActorCritic(state_dim, action_dim)\n",
    "ppo_agent = PPO(agent)\n",
    "\n",
    "# 生成模拟数据并进行一次更新\n",
    "states, actions, rewards, dones, old_log_probs, values = generate_fake_data(agent, 100, state_dim, action_dim)\n",
    "training_stats = ppo_agent.update(states, actions, rewards, dones, old_log_probs, values)\n",
    "\n",
    "print(\"Training Statistics:\", training_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
