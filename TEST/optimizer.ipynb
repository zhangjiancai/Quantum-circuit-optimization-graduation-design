{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义PPO类，实现近端策略优化算法\n",
    "\n",
    "class PPO:\n",
    "    \"\"\"\n",
    "    近端策略优化（Proximal Policy Optimization，PPO）算法。\n",
    "\n",
    "    一种强化学习算法，通过信任区域方法进行策略更新，平衡探索与利用。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent, lr=3e-4, gamma=0.99, clip_epsilon=0.2):\n",
    "        \"\"\"\n",
    "        初始化PPO算法。\n",
    "\n",
    "        参数:\n",
    "        - agent: 强化学习智能体模型，通常为神经网络\n",
    "        - lr: 优化器的学习率\n",
    "        - gamma: 未来奖励的折现因子\n",
    "        - clip_epsilon: PPO中用于限制策略更新的裁剪参数\n",
    "        \"\"\"\n",
    "        self.agent = agent  # 设置智能体模型\n",
    "        self.optimizer = torch.optim.Adam(agent.parameters(), lr=lr)  # 使用Adam优化器调整模型参数\n",
    "        self.gamma = gamma  # 折现因子，用于计算未来奖励的当前价值\n",
    "        self.clip_epsilon = clip_epsilon  # 策略更新时的裁剪范围界限\n",
    "\n",
    "    def update(self, states, actions, rewards, dones, old_log_probs, values):\n",
    "        \"\"\"\n",
    "        根据收集的经验数据更新PPO智能体。\n",
    "\n",
    "        步骤包括计算广义优势估计（GAE）、新旧策略的比率、策略损失、价值损失以及加入熵正则化项的总损失，\n",
    "        并执行反向传播和参数更新。\n",
    "        \"\"\"\n",
    "        # 计算广义优势估计（GAE）和回报\n",
    "        returns, advantages = self.compute_gae(rewards, dones, values)\n",
    "\n",
    "        # 从当前策略中获取新的动作分布和状态价值\n",
    "        new_policy, new_values = self.agent(states)\n",
    "        print(\"new_policy:\", new_policy.shape)\n",
    "        print(\"new_values:\", new_values.shape)\n",
    "        print(\"actions:\", actions.shape)\n",
    "        # 计算新策略下动作的对数概率\n",
    "        new_log_probs = self.compute_log_probs(new_policy, actions)\n",
    "        print(\"new_log_probs:\", new_log_probs.shape)\n",
    "\n",
    "        # 计算重要性采样比值（ratio）\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "        # 对优势进行标准化处理，以稳定训练\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # 计算损失函数的两部分：策略损失（通过裁剪）和价值函数损失（可选的裁剪形式）\n",
    "        # 假设surr1和surr2的计算方式\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # 价值函数损失计算，考虑了裁剪以减少更新波动\n",
    "        # 假设returns的原始形状是[batch_size, sequence_length]\n",
    "        returns = returns.sum(dim=1, keepdim=True)  # 累加sequence_length维度，添加keepdim保持二维结构\n",
    "        # 假设new_values的原始形状是[batch_size, sequence_length]\n",
    "        new_values_aggregated = new_values.sum(dim=1, keepdim=True)  # 累加sequence_length维度，保持二维结构\n",
    "        value_loss = F.mse_loss(new_values_aggregated, returns)\n",
    "\n",
    "        # 计算策略熵，鼓励探索\n",
    "        new_policy_normalized = F.softmax(new_policy, dim=-1)\n",
    "        # 现在使用归一化后的概率分布来计算熵\n",
    "        entropy = Categorical(probs=new_policy_normalized).entropy().mean()\n",
    "\n",
    "        # 总损失，结合策略损失、价值损失和熵项\n",
    "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        # 执行优化步骤：梯度清零、反向传播、参数更新\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # 在update方法的末尾添加以下代码行来输出各个数据的维度\n",
    "        print(\"States dimension:\", states.shape)\n",
    "        print(\"Actions dimension:\", actions.shape)\n",
    "        print(\"Advantages dimension:\", advantages.shape)\n",
    "        print(\"Returns dimension:\", returns.shape)\n",
    "        print(\"New Policy logits dimension:\", new_policy.shape)\n",
    "        print(\"New Values dimension:\", new_values.shape)\n",
    "        print(\"New Log Probs dimension:\", new_log_probs.shape)\n",
    "\n",
    "        # 返回训练统计信息，供监控和调试\n",
    "        return {\n",
    "            '策略损失': policy_loss.item(),\n",
    "            '价值损失': value_loss.item(),\n",
    "            '熵': entropy.item(),\n",
    "            '总损失': total_loss.item(),\n",
    "            '平均比率': ratio.mean().item()\n",
    "        }\n",
    "\n",
    "    def compute_gae(self, rewards, dones, values):\n",
    "        \"\"\"\n",
    "        计算每个时间步的广义优势估计（Generalized Advantage Estimation, GAE）。\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        gae_lambda = 0.95  # GAE中的衰减因子λ\n",
    "        last_gae_lam = 0\n",
    "\n",
    "        # 修改循环，使其不包括最后一个元素，避免索引越界\n",
    "        for t in reversed(range(len(rewards) - 1)):  # 减1以避免索引越界\n",
    "            next_non_terminal = 1.0 - dones[t+1]\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * next_non_terminal - values[t]\n",
    "            advantages[t] = delta + self.gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            last_gae_lam = advantages[t]\n",
    "\n",
    "        # 对于序列的第一个元素（t=0），我们需要特殊处理，这里直接设置为累计的优势\n",
    "        # 注意，因为我们已经处理到了t=1（原序列的倒数第二个元素），所以不需要额外的循环迭代\n",
    "        #advantages[0] = advantages[0]  # 这一行实际上不需要操作，因为第一个元素的advantage已经在循环中计算好\n",
    "\n",
    "        returns = advantages + values\n",
    "        return returns, advantages\n",
    "\n",
    "    def compute_log_probs(self, policy_dist, actions):\n",
    "        \"\"\"\n",
    "        计算在给定动作分布下选择的动作的对数概率。\n",
    "\n",
    "        确保在传递给Categorical分布之前进行规范化。\n",
    "        \"\"\"\n",
    "        # 计算未规范化概率\n",
    "        unnormalized_probs = torch.exp(policy_dist)\n",
    "\n",
    "        # 规范化概率以满足概率分布的约束\n",
    "        probs = unnormalized_probs / unnormalized_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # 处理可能出现的NaN或无穷大值\n",
    "        # 将它们设为0以避免数值问题\n",
    "        probs[torch.isnan(probs)] = 0.0\n",
    "        probs[probs == float('inf')] = 0.0\n",
    "\n",
    "        # 创建Categorical分布对象并计算对数概率\n",
    "        dist = Categorical(probs=probs)\n",
    "        log_probs = dist.log_prob(actions)  # 计算对数概率\n",
    "\n",
    "        return log_probs\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_policy: torch.Size([100, 2])\n",
      "new_values: torch.Size([100, 1, 1])\n",
      "actions: torch.Size([100])\n",
      "new_log_probs: torch.Size([100])\n",
      "States dimension: torch.Size([100, 4])\n",
      "Actions dimension: torch.Size([100])\n",
      "Advantages dimension: torch.Size([100, 1])\n",
      "Returns dimension: torch.Size([100, 1, 1])\n",
      "New Policy logits dimension: torch.Size([100, 2])\n",
      "New Values dimension: torch.Size([100, 1, 1])\n",
      "New Log Probs dimension: torch.Size([100])\n",
      "Training Statistics: {'策略损失': 1.3971328982620435e-08, '价值损失': 44751.0859375, '熵': 0.6588541269302368, '总损失': 22375.537109375, '平均比率': 1.0}\n",
      "States dimension: torch.Size([100, 4])\n",
      "Actions dimension: torch.Size([100])\n",
      "Rewards dimension: torch.Size([100, 1])\n",
      "Dones dimension: torch.Size([100, 1])\n",
      "Old log probs dimension: torch.Size([100])\n",
      "Values dimension: torch.Size([100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# 构建一个简单的智能体模型作为示例\n",
    "class SimpleActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(SimpleActorCritic, self).__init__()\n",
    "        self.shared_layers = nn.Linear(state_dim, 128)\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_out = torch.relu(self.shared_layers(x))\n",
    "        policy_logits = self.policy_head(shared_out)\n",
    "        state_values = self.value_head(shared_out)\n",
    "        return policy_logits, state_values.unsqueeze(-1)\n",
    "\n",
    "# 生成模拟数据\n",
    "def generate_fake_data(agent, num_steps, state_dim, action_dim):\n",
    "    states = torch.randn(num_steps, state_dim)\n",
    "    actions = Categorical(logits=agent(states)[0]).sample()\n",
    "    rewards = torch.randn(num_steps, 1)\n",
    "    dones = torch.zeros(num_steps, 1, dtype=torch.float)\n",
    "    dones[-1] = 1  # 最后一步设置为终止状态\n",
    "    values = agent(states)[1]\n",
    "    old_log_probs = Categorical(logits=agent(states)[0]).log_prob(actions)\n",
    "    return states, actions, rewards, dones, old_log_probs, values\n",
    "\n",
    "# 实例化PPO类和智能体模型\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = SimpleActorCritic(state_dim, action_dim)\n",
    "ppo_agent = PPO(agent)\n",
    "\n",
    "# 生成模拟数据并进行一次更新\n",
    "states, actions, rewards, dones, old_log_probs, values = generate_fake_data(agent, 100, state_dim, action_dim)\n",
    "training_stats = ppo_agent.update(states, actions, rewards, dones, old_log_probs, values)\n",
    "\n",
    "print(\"Training Statistics:\", training_stats)\n",
    "print(\"States dimension:\", states.shape)\n",
    "print(\"Actions dimension:\", actions.shape)\n",
    "print(\"Rewards dimension:\", rewards.shape)\n",
    "print(\"Dones dimension:\", dones.shape)\n",
    "print(\"Old log probs dimension:\", old_log_probs.shape)\n",
    "print(\"Values dimension:\", values.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
