{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# 定义PPO类，实现近端策略优化算法\n",
    "\n",
    "class PPO:\n",
    "    \"\"\"\n",
    "    近端策略优化（Proximal Policy Optimization，PPO）算法。\n",
    "\n",
    "    一种强化学习算法，通过信任区域方法进行策略更新，平衡探索与利用。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent, lr=3e-4, gamma=0.99, clip_epsilon=0.2):\n",
    "        \"\"\"\n",
    "        初始化PPO算法。\n",
    "\n",
    "        参数:\n",
    "        - agent: 强化学习智能体模型，通常为神经网络\n",
    "        - lr: 优化器的学习率\n",
    "        - gamma: 未来奖励的折现因子\n",
    "        - clip_epsilon: PPO中用于限制策略更新的裁剪参数\n",
    "        \"\"\"\n",
    "        self.agent = agent  # 设置智能体模型\n",
    "        self.optimizer = torch.optim.Adam(agent.parameters(), lr=lr)  # 使用Adam优化器调整模型参数\n",
    "        self.gamma = gamma  # 折现因子，用于计算未来奖励的当前价值\n",
    "        self.clip_epsilon = clip_epsilon  # 策略更新时的裁剪范围界限\n",
    "\n",
    "    def update(self, states, actions, rewards, dones, old_log_probs, values):\n",
    "        \"\"\"\n",
    "        根据收集的经验数据更新PPO智能体。\n",
    "\n",
    "        步骤包括计算广义优势估计（GAE）、新旧策略的比率、策略损失、价值损失以及加入熵正则化项的总损失，\n",
    "        并执行反向传播和参数更新。\n",
    "        \"\"\"\n",
    "        # 计算广义优势估计（GAE）和回报\n",
    "        returns, advantages = self.compute_gae(rewards, dones, values)\n",
    "\n",
    "        # 从当前策略中获取新的动作分布和状态价值\n",
    "        new_policy, new_values = self.agent(states)\n",
    "\n",
    "        # 计算新策略下动作的对数概率\n",
    "        new_log_probs = self.compute_log_probs(new_policy, actions)\n",
    "\n",
    "        # 计算重要性采样比值（ratio）\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "        # 假设advantages是(batch_size, )或(batch_size, sequence_length)的形状\n",
    "        mean_advantages = advantages.mean(dim=0 if advantages.dim() == 2 else None)  # 正确处理一维或二维张量\n",
    "        std_advantages = advantages.std(dim=0 if advantages.dim() == 2 else None, unbiased=False)  # 同上，unbiased=False忽略样本数减1\n",
    "        advantages = (advantages - mean_advantages) / (std_advantages + 1e-8)\n",
    "\n",
    "        # 计算损失函数的两部分：策略损失（通过裁剪）和价值函数损失（可选的裁剪形式）\n",
    "        # 假设surr1和surr2的计算方式\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # 价值函数损失计算，考虑了裁剪以减少更新波动\n",
    "        # 假设returns的原始形状是[batch_size, sequence_length]\n",
    "        returns = returns.sum(dim=0)  # 累加sequence_length维度\n",
    "        #returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(0)  # 如果returns是Python标量，转换为张量并增加一维\n",
    "        returns = returns.clone().detach().unsqueeze(0) #这段代码首先使用.clone()创建returns张量的一个副本，这样就断开了与原始计算图的连接。然后，使用.detach()确保新创建的张量不会跟踪任何历史梯度。最后，unsqueeze(0)用于在张量的最前面添加一个维度。这样，即使returns是一个标量，也能得到一个形状为(1, sequence_length)的张量。\n",
    "        \n",
    "        # 假设new_values的原始形状是[batch_size, sequence_length]\n",
    "        new_values_aggregated = new_values.sum(dim=0)  # 累加sequence_length维度\n",
    "        value_loss = F.mse_loss(new_values_aggregated, returns)\n",
    "        # 修正 `value_loss` 计算\n",
    "        #value_loss = F.mse_loss(new_values.squeeze(), returns)\n",
    "        # 计算策略熵，鼓励探索\n",
    "        new_policy_normalized = F.softmax(new_policy, dim=-1)\n",
    "        # 现在使用归一化后的概率分布来计算熵\n",
    "        entropy = Categorical(probs=new_policy_normalized).entropy().mean()\n",
    "\n",
    "        # 总损失，结合策略损失、价值损失和熵项\n",
    "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        # 执行优化步骤：梯度清零、反向传播、参数更新\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 返回训练统计信息，供监控和调试\n",
    "        return {\n",
    "            '策略损失': policy_loss.item(),\n",
    "            '价值损失': value_loss.item(),\n",
    "            '熵': entropy.item(),\n",
    "            '总损失': total_loss.item(),\n",
    "            '平均比率': ratio.mean().item()\n",
    "        }\n",
    "\n",
    "    def compute_gae(self, rewards, dones, values):\n",
    "        \"\"\"\n",
    "        计算每个时间步的广义优势估计（Generalized Advantage Estimation, GAE）。\n",
    "        \"\"\"\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        gae_lambda = 0.95  # GAE中的衰减因子λ\n",
    "        last_gae_lam = 0\n",
    "\n",
    "        # 修改循环，使其不包括最后一个元素，避免索引越界\n",
    "        for t in reversed(range(len(rewards) - 1)):  # 减1以避免索引越界\n",
    "            next_non_terminal = 1.0 - dones[t+1]\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * next_non_terminal - values[t]\n",
    "            advantages[t] = delta + self.gamma * gae_lambda * next_non_terminal * last_gae_lam\n",
    "            last_gae_lam = advantages[t]\n",
    "\n",
    "        # 对于序列的第一个元素（t=0），我们需要特殊处理，这里直接设置为累计的优势\n",
    "        # 注意，因为我们已经处理到了t=1（原序列的倒数第二个元素），所以不需要额外的循环迭代\n",
    "        #advantages[0] = advantages[0]  # 这一行实际上不需要操作，因为第一个元素的advantage已经在循环中计算好\n",
    "\n",
    "        returns = advantages + values\n",
    "        return returns, advantages\n",
    "\n",
    "    def compute_log_probs(self,policy, actions):\n",
    "        \"\"\"\n",
    "            计算在给定动作分布下选择的动作的对数概率。\n",
    "    \n",
    "        参数：\n",
    "            policy (torch.Tensor): 策略输出，形状为 [batch_size, n_rules, n_qubits * n_moments]\n",
    "            actions (torch.Tensor): 动作索引，形状为 [batch_size, 2]，其中包括规则索引和量子位时刻索引。\n",
    "        \"\"\"\n",
    "        # 提取规则索引和量子位时刻索引\n",
    "        rule_indices = actions[:, 0]\n",
    "        qubit_moment_indices = actions[:, 1]\n",
    "\n",
    "        # 批次大小\n",
    "        batch_size = policy.shape[0]\n",
    "\n",
    "        # 收集对应的概率\n",
    "        log_probs = torch.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            # 获取单个样本的规则对应的概率分布\n",
    "            rule_prob = policy[i, rule_indices[i]]\n",
    "\n",
    "            # 计算未规范化概率\n",
    "            unnormalized_probs = torch.exp(rule_prob)\n",
    "\n",
    "            # 规范化概率以满足概率分布的约束\n",
    "            probs = unnormalized_probs / unnormalized_probs.sum(dim=0, keepdim=True)\n",
    "\n",
    "            # 处理可能出现的NaN或无穷大值\n",
    "            probs[torch.isnan(probs)] = 0.0\n",
    "            probs[probs == float('inf')] = 0.0\n",
    "\n",
    "            # 创建Categorical分布对象并计算对数概率\n",
    "            dist = Categorical(probs=probs)\n",
    "            log_prob = dist.log_prob(qubit_moment_indices[i])\n",
    "            log_probs[i] = log_prob\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValues: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mtest_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mtest_ppo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# 修正values的长度为5，与其它输入匹配\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 更新代理模型\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_log_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 打印一些测试数据\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO Update Test Complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m, in \u001b[0;36mPPO.update\u001b[1;34m(self, states, actions, rewards, dones, old_log_probs, values)\u001b[0m\n\u001b[0;32m     43\u001b[0m new_policy, new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent(states)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 计算新策略下动作的对数概率\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m new_log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_log_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 计算重要性采样比值（ratio）\u001b[39;00m\n\u001b[0;32m     49\u001b[0m ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(new_log_probs \u001b[38;5;241m-\u001b[39m old_log_probs)\n",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m, in \u001b[0;36mPPO.compute_log_probs\u001b[1;34m(self, policy, actions)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    计算在给定动作分布下选择的动作的对数概率。\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    actions (torch.Tensor): 动作索引，形状为 [batch_size, 2]，其中包括规则索引和量子位时刻索引。\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# 提取规则索引和量子位时刻索引\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m rule_indices \u001b[38;5;241m=\u001b[39m \u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    127\u001b[0m qubit_moment_indices \u001b[38;5;241m=\u001b[39m actions[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# 批次大小\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "class DummyAgent(nn.Module):\n",
    "    \"\"\"用于测试的简易代理模型\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DummyAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2_policy = nn.Linear(20, 3)\n",
    "        self.fc2_value = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        policy = F.softmax(self.fc2_policy(x), dim=-1)\n",
    "        value = self.fc2_value(x)\n",
    "        return policy, value\n",
    "\n",
    "def test_ppo():\n",
    "    \"\"\"测试 PPO 类\"\"\"\n",
    "    # 创建测试数据\n",
    "    agent = DummyAgent()\n",
    "    ppo = PPO(agent)\n",
    "    states = torch.randn(5, 10)\n",
    "    actions = torch.tensor([0, 1, 2, 1, 0], dtype=torch.int64)\n",
    "    rewards = torch.tensor([1.0, 0.5, -0.5, 1.0, 2.0], dtype=torch.float32)\n",
    "    dones = torch.tensor([0, 0, 0, 1, 1], dtype=torch.float32)\n",
    "    old_log_probs = torch.tensor([-0.1, -0.2, -0.3, -0.4, -0.5], dtype=torch.float32)\n",
    "    values = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5], dtype=torch.float32)  # 修正values的长度为5，与其它输入匹配\n",
    "\n",
    "    # 更新代理模型\n",
    "    ppo.update(states, actions, rewards, dones, old_log_probs, values)\n",
    "\n",
    "    # 打印一些测试数据\n",
    "    print(\"PPO Update Test Complete\")\n",
    "    print(f\"States: {states}\")\n",
    "    print(f\"Actions: {actions}\")\n",
    "    print(f\"Rewards: {rewards}\")\n",
    "    print(f\"Dones: {dones}\")\n",
    "    print(f\"Old Log Probs: {old_log_probs}\")\n",
    "    print(f\"Values: {values}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_ppo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的action是有问题的。没写测试代码。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
