{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CircuitOptimizerAgent(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(CircuitOptimizerAgent, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_gate_types = num_gate_types\n",
    "        self.num_transform_rules = num_transform_rules\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # 3D卷积层\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=3, padding=1)  # 输入通道2，输出通道16，核大小3x3x3，填充1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 使用2x2x2的池化窗口\n",
    "\n",
    "        # 计算池化后的维度\n",
    "        pooled_dim = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "\n",
    "        # 策略网络的全连接层\n",
    "        self.fc1_policy = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2_policy = nn.Linear(256, num_qubits * num_transform_rules * num_timesteps)\n",
    "\n",
    "        # 价值网络的全连接层\n",
    "        self.fc1_value = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2_value = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 共享的卷积层\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "\n",
    "        # 为全连接层展平数据\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "\n",
    "        # 策略网络\n",
    "        policy = F.relu(self.fc1_policy(x_flat))\n",
    "        policy = self.fc2_policy(policy)\n",
    "        policy = policy.view(-1, self.num_qubits, self.num_transform_rules, self.num_timesteps)\n",
    "        policy = F.softmax(policy, dim=-1)\n",
    "\n",
    "        # 价值网络\n",
    "        value = F.relu(self.fc1_value(x_flat))\n",
    "        value = self.fc2_value(value)\n",
    "\n",
    "        return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 6\n",
    "num_timesteps = 15\n",
    "activation_probability = 0.1  # 设定任一门在任一位置激活的概率\n",
    "agent = CircuitOptimizerAgent(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化四维数组，最后一维表示是否激活\n",
    "quantum_circuit_data = np.zeros((num_qubits, num_gate_types, num_timesteps, 2))\n",
    "\n",
    "# 随机选择一些位置设置为1，模拟量子门的激活\n",
    "np.random.seed(42)  # 设置随机种子以确保可复现性\n",
    "for qubit in range(num_qubits):\n",
    "    for gate_type in range(num_gate_types):\n",
    "        for time in range(num_timesteps):\n",
    "            if np.random.random() < activation_probability:\n",
    "                quantum_circuit_data[qubit, gate_type, time, 1] = 1  # 设置为1表示激活\n",
    "\n",
    "# 最后的一维是0和1，这里我们用第一个维度设置其它为0表示无激活\n",
    "quantum_circuit_data[:, :, :, 0] = 1 - quantum_circuit_data[:, :, :, 1]\n",
    "\n",
    "# 添加batch和channel维度\n",
    "data_tensor = torch.tensor(quantum_circuit_data, dtype=torch.float).unsqueeze(0).permute(0, 4, 3, 1, 2)\n",
    "# 这里用 permute 将数据调整为 (batch_size, channels, depth, height, width)\n",
    "# 对应的shape是 (1, 2, num_timesteps, num_gate_types, num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Output shape: torch.Size([1, 5, 6, 15])\n",
      "Policy Output: tensor([[[[0.0689, 0.0680, 0.0663, 0.0645, 0.0700, 0.0722, 0.0643, 0.0619,\n",
      "           0.0553, 0.0579, 0.0648, 0.0807, 0.0668, 0.0687, 0.0698],\n",
      "          [0.0673, 0.0649, 0.0577, 0.0660, 0.0724, 0.0640, 0.0573, 0.0685,\n",
      "           0.0725, 0.0758, 0.0631, 0.0612, 0.0776, 0.0615, 0.0704],\n",
      "          [0.0632, 0.0703, 0.0824, 0.0754, 0.0578, 0.0680, 0.0557, 0.0617,\n",
      "           0.0641, 0.0717, 0.0649, 0.0719, 0.0683, 0.0633, 0.0614],\n",
      "          [0.0673, 0.0671, 0.0652, 0.0623, 0.0720, 0.0671, 0.0727, 0.0742,\n",
      "           0.0640, 0.0567, 0.0660, 0.0689, 0.0701, 0.0658, 0.0607],\n",
      "          [0.0609, 0.0641, 0.0568, 0.0728, 0.0739, 0.0574, 0.0651, 0.0675,\n",
      "           0.0639, 0.0619, 0.0647, 0.0680, 0.0664, 0.0752, 0.0814],\n",
      "          [0.0650, 0.0590, 0.0812, 0.0695, 0.0524, 0.0678, 0.0655, 0.0696,\n",
      "           0.0704, 0.0710, 0.0602, 0.0617, 0.0763, 0.0753, 0.0552]],\n",
      "\n",
      "         [[0.0646, 0.0711, 0.0677, 0.0623, 0.0712, 0.0685, 0.0580, 0.0708,\n",
      "           0.0576, 0.0803, 0.0616, 0.0752, 0.0598, 0.0688, 0.0625],\n",
      "          [0.0696, 0.0721, 0.0592, 0.0735, 0.0713, 0.0761, 0.0603, 0.0590,\n",
      "           0.0744, 0.0630, 0.0588, 0.0665, 0.0584, 0.0673, 0.0706],\n",
      "          [0.0643, 0.0561, 0.0745, 0.0635, 0.0535, 0.0544, 0.0709, 0.0768,\n",
      "           0.0783, 0.0723, 0.0650, 0.0703, 0.0713, 0.0693, 0.0595],\n",
      "          [0.0690, 0.0560, 0.0565, 0.0771, 0.0655, 0.0549, 0.0671, 0.0675,\n",
      "           0.0734, 0.0643, 0.0745, 0.0745, 0.0751, 0.0629, 0.0619],\n",
      "          [0.0705, 0.0691, 0.0546, 0.0659, 0.0600, 0.0730, 0.0713, 0.0711,\n",
      "           0.0660, 0.0679, 0.0688, 0.0585, 0.0590, 0.0780, 0.0664],\n",
      "          [0.0720, 0.0635, 0.0650, 0.0552, 0.0678, 0.0599, 0.0897, 0.0638,\n",
      "           0.0629, 0.0663, 0.0652, 0.0707, 0.0658, 0.0681, 0.0642]],\n",
      "\n",
      "         [[0.0607, 0.0644, 0.0720, 0.0642, 0.0712, 0.0598, 0.0833, 0.0645,\n",
      "           0.0617, 0.0728, 0.0643, 0.0702, 0.0610, 0.0722, 0.0576],\n",
      "          [0.0676, 0.0626, 0.0621, 0.0624, 0.0755, 0.0699, 0.0542, 0.0671,\n",
      "           0.0617, 0.0735, 0.0664, 0.0704, 0.0731, 0.0704, 0.0631],\n",
      "          [0.0789, 0.0659, 0.0572, 0.0893, 0.0625, 0.0639, 0.0637, 0.0608,\n",
      "           0.0680, 0.0710, 0.0601, 0.0618, 0.0653, 0.0698, 0.0619],\n",
      "          [0.0645, 0.0618, 0.0602, 0.0732, 0.0655, 0.0606, 0.0723, 0.0745,\n",
      "           0.0657, 0.0606, 0.0639, 0.0625, 0.0752, 0.0672, 0.0724],\n",
      "          [0.0609, 0.0685, 0.0738, 0.0710, 0.0658, 0.0612, 0.0664, 0.0733,\n",
      "           0.0680, 0.0693, 0.0769, 0.0613, 0.0612, 0.0531, 0.0691],\n",
      "          [0.0620, 0.0753, 0.0696, 0.0696, 0.0608, 0.0693, 0.0655, 0.0647,\n",
      "           0.0644, 0.0641, 0.0580, 0.0710, 0.0700, 0.0698, 0.0659]],\n",
      "\n",
      "         [[0.0700, 0.0764, 0.0623, 0.0730, 0.0713, 0.0578, 0.0499, 0.0704,\n",
      "           0.0705, 0.0722, 0.0531, 0.0718, 0.0602, 0.0677, 0.0735],\n",
      "          [0.0633, 0.0736, 0.0528, 0.0691, 0.0601, 0.0669, 0.0681, 0.0733,\n",
      "           0.0777, 0.0783, 0.0659, 0.0710, 0.0538, 0.0599, 0.0661],\n",
      "          [0.0660, 0.0620, 0.0772, 0.0649, 0.0811, 0.0632, 0.0685, 0.0684,\n",
      "           0.0592, 0.0646, 0.0664, 0.0630, 0.0633, 0.0666, 0.0656],\n",
      "          [0.0758, 0.0599, 0.0676, 0.0609, 0.0690, 0.0662, 0.0626, 0.0713,\n",
      "           0.0730, 0.0792, 0.0649, 0.0667, 0.0594, 0.0620, 0.0616],\n",
      "          [0.0641, 0.0714, 0.0636, 0.0776, 0.0593, 0.0680, 0.0652, 0.0532,\n",
      "           0.0673, 0.0624, 0.0654, 0.0663, 0.0652, 0.0693, 0.0816],\n",
      "          [0.0634, 0.0713, 0.0746, 0.0711, 0.0732, 0.0655, 0.0692, 0.0652,\n",
      "           0.0643, 0.0618, 0.0693, 0.0608, 0.0616, 0.0684, 0.0604]],\n",
      "\n",
      "         [[0.0743, 0.0538, 0.0700, 0.0702, 0.0593, 0.0620, 0.0846, 0.0635,\n",
      "           0.0722, 0.0548, 0.0697, 0.0643, 0.0744, 0.0652, 0.0616],\n",
      "          [0.0768, 0.0677, 0.0648, 0.0774, 0.0659, 0.0614, 0.0663, 0.0642,\n",
      "           0.0748, 0.0598, 0.0625, 0.0589, 0.0571, 0.0691, 0.0731],\n",
      "          [0.0690, 0.0674, 0.0624, 0.0675, 0.0623, 0.0645, 0.0717, 0.0758,\n",
      "           0.0714, 0.0612, 0.0663, 0.0613, 0.0707, 0.0606, 0.0679],\n",
      "          [0.0825, 0.0722, 0.0541, 0.0630, 0.0605, 0.0698, 0.0646, 0.0709,\n",
      "           0.0680, 0.0608, 0.0672, 0.0696, 0.0655, 0.0566, 0.0745],\n",
      "          [0.0790, 0.0623, 0.0643, 0.0642, 0.0614, 0.0757, 0.0657, 0.0731,\n",
      "           0.0701, 0.0620, 0.0621, 0.0654, 0.0614, 0.0746, 0.0586],\n",
      "          [0.0805, 0.0688, 0.0560, 0.0706, 0.0608, 0.0579, 0.0565, 0.0624,\n",
      "           0.0625, 0.0644, 0.0743, 0.0808, 0.0659, 0.0745, 0.0642]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Value Output shape: torch.Size([1, 1])\n",
      "Value Output: tensor([[0.0315]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 运行模型\n",
    "policy_output, value_output = agent(data_tensor)\n",
    "print(\"Policy Output shape:\", policy_output.shape)\n",
    "print(\"Policy Output:\", policy_output)\n",
    "print(\"Value Output shape:\", value_output.shape)\n",
    "print(\"Value Output:\", value_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 设置量子电路数据的参数\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_timesteps = 10\n",
    "activation_probability = 0.1  # 设定任一门在任一位置激活的概率\n",
    "\n",
    "# 初始化四维数组，最后一维表示是否激活\n",
    "quantum_circuit_data = np.zeros((num_qubits, num_gate_types, num_timesteps, 2))\n",
    "\n",
    "# 随机选择一些位置设置为1，模拟量子门的激活\n",
    "np.random.seed(42)  # 设置随机种子以确保可复现性\n",
    "for qubit in range(num_qubits):\n",
    "    for gate_type in range(num_gate_types):\n",
    "        for time in range(num_timesteps):\n",
    "            if np.random.random() < activation_probability:\n",
    "                quantum_circuit_data[qubit, gate_type, time, 1] = 1  # 设置为1表示激活\n",
    "\n",
    "# 最后的一维是0和1，这里我们用第一个维度设置其它为0表示无激活\n",
    "quantum_circuit_data[:, :, :, 0] = 1 - quantum_circuit_data[:, :, :, 1]\n",
    "\n",
    "# 添加batch和channel维度\n",
    "data_tensor = torch.tensor(quantum_circuit_data, dtype=torch.float).unsqueeze(0).permute(0, 4, 3, 1, 2)\n",
    "# 这里用 permute 将数据调整为 (batch_size, channels, depth, height, width)\n",
    "# 对应的shape是 (1, 2, num_timesteps, num_gate_types, num_qubits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_gate_types = num_gate_types\n",
    "        self.num_transform_rules = num_transform_rules\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # 3D卷积层\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=3, padding=1)  # 输入通道2，输出通道16，核大小3x3x3，填充1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 池化窗口2x2x2\n",
    "\n",
    "        # 计算池化后的维度\n",
    "        pooled_dim = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "        self.fc1 = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平操作，为全连接层准备\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, self.num_qubits, self.num_gate_types, self.num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 5, 3, 10])\n",
      "Policy Output: tensor([[[[0.1039, 0.0996, 0.1015, 0.1149, 0.0973, 0.1027, 0.0922, 0.0999,\n",
      "           0.0970, 0.0909],\n",
      "          [0.0960, 0.0880, 0.1102, 0.0994, 0.1076, 0.0843, 0.0957, 0.1180,\n",
      "           0.0978, 0.1029],\n",
      "          [0.1164, 0.1072, 0.0915, 0.0997, 0.1067, 0.0917, 0.0944, 0.1035,\n",
      "           0.1020, 0.0869]],\n",
      "\n",
      "         [[0.0892, 0.0999, 0.1040, 0.1069, 0.1001, 0.0884, 0.0951, 0.1065,\n",
      "           0.1062, 0.1036],\n",
      "          [0.1023, 0.0918, 0.1104, 0.1068, 0.0866, 0.1189, 0.0995, 0.0929,\n",
      "           0.0973, 0.0935],\n",
      "          [0.1064, 0.1147, 0.1114, 0.1080, 0.0966, 0.1049, 0.0880, 0.0808,\n",
      "           0.0903, 0.0989]],\n",
      "\n",
      "         [[0.1012, 0.1092, 0.0961, 0.0950, 0.0989, 0.0998, 0.1028, 0.1072,\n",
      "           0.0940, 0.0958],\n",
      "          [0.1124, 0.0977, 0.0879, 0.1049, 0.0974, 0.0934, 0.1013, 0.1004,\n",
      "           0.1056, 0.0991],\n",
      "          [0.0993, 0.1086, 0.0947, 0.0943, 0.0872, 0.0963, 0.1019, 0.1114,\n",
      "           0.1011, 0.1052]],\n",
      "\n",
      "         [[0.1025, 0.1088, 0.1093, 0.0990, 0.0762, 0.1057, 0.1048, 0.0943,\n",
      "           0.0945, 0.1049],\n",
      "          [0.0866, 0.0976, 0.1106, 0.1034, 0.0818, 0.0973, 0.1144, 0.1018,\n",
      "           0.0967, 0.1099],\n",
      "          [0.0983, 0.1116, 0.1196, 0.1153, 0.0879, 0.0971, 0.1006, 0.0940,\n",
      "           0.0836, 0.0921]],\n",
      "\n",
      "         [[0.1029, 0.1023, 0.0832, 0.1013, 0.1089, 0.1223, 0.0887, 0.1059,\n",
      "           0.0918, 0.0926],\n",
      "          [0.1071, 0.0891, 0.0907, 0.0991, 0.0880, 0.1094, 0.0958, 0.1301,\n",
      "           0.0982, 0.0924],\n",
      "          [0.0978, 0.1026, 0.0939, 0.0865, 0.1011, 0.1061, 0.1083, 0.1033,\n",
      "           0.1100, 0.0902]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 10\n",
    "num_timesteps = 10\n",
    "\n",
    "# 实例化策略网络\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n",
    "\n",
    "# 运行模型\n",
    "output = policy_net(data_tensor)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Policy Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保`num_qubits`, `num_gate_types`, `num_transform_rules`等参数与之前的数据结构一一对应非常重要。以下是一个更详细的代码说明，它们是如何在整个过程中保持一致的：\n",
    "\n",
    "### 数据生成与准备\n",
    "\n",
    "首先，我们要确保生成的量子电路数据与模型所期望的输入格式相匹配。这个过程涉及初始化一个四维数组来表示量子比特、量子门类型、时间步数，以及激活状态。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 设置量子电路数据的参数\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_timesteps = 10\n",
    "activation_probability = 0.1  # 设定任一门在任一位置激活的概率\n",
    "\n",
    "# 初始化四维数组，最后一维表示是否激活\n",
    "quantum_circuit_data = np.zeros((num_qubits, num_gate_types, num_timesteps, 2))\n",
    "\n",
    "# 随机选择一些位置设置为1，模拟量子门的激活\n",
    "np.random.seed(42)  # 设置随机种子以确保可复现性\n",
    "for qubit in range(num_qubits):\n",
    "    for gate_type in range(num_gate_types):\n",
    "        for time in range(num_timesteps):\n",
    "            if np.random.random() < activation_probability:\n",
    "                quantum_circuit_data[qubit, gate_type, time, 1] = 1  # 设置为1表示激活\n",
    "\n",
    "# 最后的一维是0和1，这里我们用第一个维度设置其它为0表示无激活\n",
    "quantum_circuit_data[:, :, :, 0] = 1 - quantum_circuit_data[:, :, :, 1]\n",
    "\n",
    "# 添加batch和channel维度\n",
    "data_tensor = torch.tensor(quantum_circuit_data, dtype=torch.float).unsqueeze(0).permute(0, 4, 3, 1, 2)\n",
    "# 这里用 permute 将数据调整为 (batch_size, channels, depth, height, width)\n",
    "# 对应的shape是 (1, 2, num_timesteps, num_gate_types, num_qubits)\n",
    "```\n",
    "\n",
    "### 定义3D策略网络\n",
    "\n",
    "策略网络的设计要匹配数据的维度和结构，确保输入数据能够正确处理并输出正确形状的概率分布。\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_gate_types = num_gate_types\n",
    "        self.num_transform_rules = num_transform_rules\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # 3D卷积层\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=3, padding=1)  # 输入通道2，输出通道16，核大小3x3x3，填充1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 池化窗口2x2x2\n",
    "\n",
    "        # 计算池化后的维度\n",
    "        pooled_dim = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "        self.fc1 = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平操作，为全连接层准备\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, self.num_qubits, self.num_gate_types, self.num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)\n",
    "```\n",
    "\n",
    "### 实例化模型并测试\n",
    "\n",
    "确保在实例化模型和准备输入数据时，所有的参数都一致。\n",
    "\n",
    "```python\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 10\n",
    "num_timesteps = 10\n",
    "\n",
    "# 实例化策略网络\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n",
    "\n",
    "# 运行模型\n",
    "output = policy_net(data_tensor)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Policy Output:\", output)\n",
    "```\n",
    "\n",
    "### 解释和验证\n",
    "\n",
    "1. **数据生成**：\n",
    "   - 我们生成了一个四维数组`quantum_circuit_data`，其形状为 `(num_qubits, num_gate_types, num_timesteps, 2)`。\n",
    "   - 最后一维表示是否激活的状态。\n",
    "   - 通过`unsqueeze(0)`添加批次维度，通过`permute`将数据调整为 `(batch_size, channels, depth, height, width)`。\n",
    "\n",
    "2. **模型设计**：\n",
    "   - 卷积层`conv1`处理输入数据，将其转换为16个通道的特征图。\n",
    "   - 池化层`pool`降低维度，提高计算效率。\n",
    "   - 全连接层`fc1`和`fc2`用于将卷积后的特征映射到策略空间（变换规则的概率分布）。\n",
    "\n",
    "3. **一致性保证**：\n",
    "   - 在数据生成、模型实例化和前向传播过程中，所有参数（如`num_qubits`, `num_gate_types`, `num_transform_rules`, `num_timesteps`）保持一致，确保数据和模型的维度匹配。\n",
    "\n",
    "通过这种方式，可以确保模型正确处理和理解量子电路数据，并输出相应的操作策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个输出是策略网络（Policy Network）生成的概率分布张量，它表示在每个量子比特、量子门类别和变换规则下采取不同操作的概率。让我们逐层解释这个张量的结构和含义。\n",
    "\n",
    "### 张量的结构和含义\n",
    "\n",
    "1. **总体结构**:\n",
    "   - 输出的张量形状是 `[1, num_qubits, num_gate_types, num_transform_rules]`，表示一个批次中的所有数据的概率分布。\n",
    "   - 在具体的示例中，`num_qubits = 5`，`num_gate_types = 3`，`num_transform_rules = 10`。\n",
    "\n",
    "2. **具体解释**:\n",
    "   - `1`: 表示批次大小，这里是1，表示我们处理一个样本。\n",
    "   - `num_qubits = 5`: 表示量子比特的数量。在这个张量中有5个切片，每个切片对应一个量子比特。\n",
    "   - `num_gate_types = 3`: 表示量子门的类别数量。每个量子比特下有3个切片，每个切片对应一个量子门类别。\n",
    "   - `num_transform_rules = 10`: 表示变换规则的数量。在每个量子比特和量子门类别的组合下，有10个值，每个值对应一个变换规则的概率。\n",
    "\n",
    "### 示例输出的具体解释\n",
    "\n",
    "考虑输出中的一个示例切片：\n",
    "\n",
    "```plaintext\n",
    "Policy Output: tensor([[[[0.0965, 0.1002, 0.0988, 0.0847, 0.0906, 0.1257, 0.1106, 0.0992, 0.1051, 0.0887],\n",
    "                        [0.0986, 0.0923, 0.1119, 0.1133, 0.0832, 0.0842, 0.0942, 0.0983, 0.1087, 0.1153],\n",
    "                        [0.0798, 0.0791, 0.1135, 0.0975, 0.1294, 0.1040, 0.0893, 0.0996, 0.1098, 0.0982]],\n",
    "\n",
    "                       [[0.0839, 0.1187, 0.0987, 0.1165, 0.0948, 0.0904, 0.1067, 0.0972, 0.1063, 0.0868],\n",
    "                        [0.0818, 0.1040, 0.1112, 0.1035, 0.0896, 0.1042, 0.0954, 0.0980, 0.1057, 0.1067],\n",
    "                        [0.1023, 0.0915, 0.0888, 0.1134, 0.0938, 0.0885, 0.1039, 0.1103, 0.1111, 0.0964]],\n",
    "\n",
    "                       [[0.1034, 0.1086, 0.1183, 0.1005, 0.0836, 0.0905, 0.1020, 0.1053, 0.0872, 0.1006],\n",
    "                        [0.0979, 0.0913, 0.0994, 0.0972, 0.1147, 0.0958, 0.0982, 0.0906, 0.1103, 0.1048],\n",
    "                        [0.1128, 0.1277, 0.0836, 0.0935, 0.0945, 0.0795, 0.1063, 0.1188, 0.0915, 0.0917]],\n",
    "\n",
    "                       [[0.0913, 0.0823, 0.1259, 0.0890, 0.1043, 0.1024, 0.0927, 0.0919, 0.1236, 0.0967],\n",
    "                        [0.0948, 0.0824, 0.0859, 0.1015, 0.1018, 0.1090, 0.1163, 0.0892, 0.1112, 0.1080],\n",
    "                        [0.0937, 0.1006, 0.1057, 0.0827, 0.0986, 0.0820, 0.1243, 0.1106, 0.0887, 0.1131]],\n",
    "\n",
    "                       [[0.1105, 0.1060, 0.1019, 0.0884, 0.0974, 0.0952, 0.0807, 0.1060, 0.0965, 0.1175],\n",
    "                        [0.1066, 0.0930, 0.1014, 0.1038, 0.0882, 0.0963, 0.1090, 0.0915, 0.1092, 0.1011],\n",
    "                        [0.0982, 0.0976, 0.1100, 0.1118, 0.1060, 0.0906, 0.1051, 0.0969, 0.0904, 0.0933]]]])\n",
    "```\n",
    "\n",
    "- **最外层列表**：表示一个批次的数据（批次大小为1）。\n",
    "- **第二层列表**：表示每个量子比特的数据（这里有5个量子比特）。\n",
    "- **第三层列表**：表示每个量子比特上的量子门类别的数据（这里每个量子比特有3种量子门类别）。\n",
    "- **第四层列表**：表示每个量子门类别下的变换规则的概率分布（这里每个量子门类别有10种变换规则）。\n",
    "\n",
    "### 例子详解\n",
    "\n",
    "```plaintext\n",
    "[[[0.0965, 0.1002, 0.0988, 0.0847, 0.0906, 0.1257, 0.1106, 0.0992, 0.1051, 0.0887],\n",
    "  [0.0986, 0.0923, 0.1119, 0.1133, 0.0832, 0.0842, 0.0942, 0.0983, 0.1087, 0.1153],\n",
    "  [0.0798, 0.0791, 0.1135, 0.0975, 0.1294, 0.1040, 0.0893, 0.0996, 0.1098, 0.0982]],\n",
    "\n",
    " [[0.0839, 0.1187, 0.0987, 0.1165, 0.0948, 0.0904, 0.1067, 0.0972, 0.1063, 0.0868],\n",
    "  [0.0818, 0.1040, 0.1112, 0.1035, 0.0896, 0.1042, 0.0954, 0.0980, 0.1057, 0.1067],\n",
    "  [0.1023, 0.0915, 0.0888, 0.1134, 0.0938, 0.0885, 0.1039, 0.1103, 0.1111, 0.0964]],\n",
    "\n",
    " [[0.1034, 0.1086, 0.1183, 0.1005, 0.0836, 0.0905, 0.1020, 0.1053, 0.0872, 0.1006],\n",
    "  [0.0979, 0.0913, 0.0994, 0.0972, 0.1147, 0.0958, 0.0982, 0.0906, 0.1103, 0.1048],\n",
    "  [0.1128, 0.1277, 0.0836, 0.0935, 0.0945, 0.0795, 0.1063, 0.1188, 0.0915, 0.0917]],\n",
    "\n",
    " [[0.0913, 0.0823, 0.1259, 0.0890, 0.1043, 0.1024, 0.0927, 0.0919, 0.1236, 0.0967],\n",
    "  [0.0948, 0.0824, 0.0859, 0.1015, 0.1018, 0.1090, 0.1163, 0.0892, 0.1112, 0.1080\n",
    "\n",
    "],\n",
    "  [0.0937, 0.1006, 0.1057, 0.0827, 0.0986, 0.0820, 0.1243, 0.1106, 0.0887, 0.1131]],\n",
    "\n",
    " [[0.1105, 0.1060, 0.1019, 0.0884, 0.0974, 0.0952, 0.0807, 0.1060, 0.0965, 0.1175],\n",
    "  [0.1066, 0.0930, 0.1014, 0.1038, 0.0882, 0.0963, 0.1090, 0.0915, 0.1092, 0.1011],\n",
    "  [0.0982, 0.0976, 0.1100, 0.1118, 0.1060, 0.0906, 0.1051, 0.0969, 0.0904, 0.0933]]]]\n",
    "```\n",
    "\n",
    "在这个示例中，每个数字表示在该量子比特、该量子门类别和该变换规则下的概率。例如，`0.0965` 表示在量子比特0，量子门类别0时，选择变换规则0的概率为9.65%。\n",
    "\n",
    "希望这些解释可以帮助您理解这个张量的结构和意义。如果有进一步的问题，请随时告诉我！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        # 输入的channel数为2，因为数据最后一维有两个通道（激活与未激活）\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=(3, 3, 3), padding=1)  # 使用3x3x3的核和padding=1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 使用2x2x2的池化窗口\n",
    "        # 计算池化后的维度\n",
    "        pool_size = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "        self.fc1 = nn.Linear(16 * pool_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平操作，为全连接层准备\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, num_qubits, num_gate_types, num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化模型\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 10\n",
    "num_timesteps = 10\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "data = np.random.rand(1, 2, num_qubits, num_gate_types, num_timesteps)  # 模拟一个batch的数据\n",
    "data_tensor = torch.tensor(data, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 5, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "# 运行模型\n",
    "output = policy_net(data_tensor)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_timesteps = 10\n",
    "num_transform_rules = 10\n",
    "batch_size = 32  # 示例批次大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps, batch_size):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        # 计算池化后的尺寸，这是必须的以确保全连接层的输入尺寸正确\n",
    "        pool_size = num_timesteps // 2 * num_qubits // 2 * num_gate_types // 2\n",
    "        self.fc1 = nn.Linear(16 * pool_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, num_qubits, num_gate_types, num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
