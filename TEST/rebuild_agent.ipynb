{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitOptimizerAgent(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(CircuitOptimizerAgent, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_gate_types = num_gate_types\n",
    "        self.num_transform_rules = num_transform_rules\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # 3D卷积层\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=3, padding=1)  # 输入通道2，输出通道16，核大小3x3x3，填充1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 使用2x2x2的池化窗口\n",
    "\n",
    "        # 计算池化后的维度\n",
    "        pooled_dim = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "\n",
    "        # 策略网络的全连接层\n",
    "        self.fc1_policy = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2_policy = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "        # 价值网络的全连接层\n",
    "        self.fc1_value = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2_value = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 共享的卷积层\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "\n",
    "        # 为全连接层展平数据\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "\n",
    "        # 策略网络\n",
    "        policy = F.relu(self.fc1_policy(x_flat))\n",
    "        policy = self.fc2_policy(policy)\n",
    "        policy = policy.view(-1, self.num_qubits, self.num_gate_types, self.num_transform_rules)\n",
    "        policy = F.softmax(policy, dim=-1)\n",
    "\n",
    "        # 价值网络\n",
    "        value = F.relu(self.fc1_value(x_flat))\n",
    "        value = self.fc2_value(value)\n",
    "\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 10\n",
    "num_timesteps = 10\n",
    "activation_probability = 0.1  # 设定任一门在任一位置激活的概率\n",
    "agent = CircuitOptimizerAgent(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化四维数组，最后一维表示是否激活\n",
    "quantum_circuit_data = np.zeros((num_qubits, num_gate_types, num_timesteps, 2))\n",
    "\n",
    "# 随机选择一些位置设置为1，模拟量子门的激活\n",
    "np.random.seed(42)  # 设置随机种子以确保可复现性\n",
    "for qubit in range(num_qubits):\n",
    "    for gate_type in range(num_gate_types):\n",
    "        for time in range(num_timesteps):\n",
    "            if np.random.random() < activation_probability:\n",
    "                quantum_circuit_data[qubit, gate_type, time, 1] = 1  # 设置为1表示激活\n",
    "\n",
    "# 最后的一维是0和1，这里我们用第一个维度设置其它为0表示无激活\n",
    "quantum_circuit_data[:, :, :, 0] = 1 - quantum_circuit_data[:, :, :, 1]\n",
    "\n",
    "# 添加batch和channel维度\n",
    "data_tensor = torch.tensor(quantum_circuit_data, dtype=torch.float).unsqueeze(0).permute(0, 4, 3, 1, 2)\n",
    "# 这里用 permute 将数据调整为 (batch_size, channels, depth, height, width)\n",
    "# 对应的shape是 (1, 2, num_timesteps, num_gate_types, num_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Output shape: torch.Size([1, 5, 3, 10])\n",
      "Policy Output: tensor([[[[0.0782, 0.1055, 0.0994, 0.1070, 0.1044, 0.0976, 0.0993, 0.1087,\n",
      "           0.0954, 0.1045],\n",
      "          [0.1072, 0.1010, 0.1237, 0.0906, 0.0809, 0.1013, 0.1019, 0.1129,\n",
      "           0.1013, 0.0791],\n",
      "          [0.1035, 0.1174, 0.0960, 0.0927, 0.0908, 0.0994, 0.0992, 0.1182,\n",
      "           0.0986, 0.0843]],\n",
      "\n",
      "         [[0.1223, 0.0906, 0.1007, 0.1135, 0.0837, 0.1026, 0.1152, 0.0922,\n",
      "           0.0934, 0.0858],\n",
      "          [0.0971, 0.1062, 0.0800, 0.1286, 0.0808, 0.0795, 0.1048, 0.1203,\n",
      "           0.1113, 0.0914],\n",
      "          [0.0829, 0.1018, 0.0842, 0.0902, 0.1271, 0.0903, 0.1015, 0.1043,\n",
      "           0.1038, 0.1140]],\n",
      "\n",
      "         [[0.1223, 0.1023, 0.1007, 0.0878, 0.1307, 0.0842, 0.0985, 0.0918,\n",
      "           0.0833, 0.0984],\n",
      "          [0.1107, 0.0952, 0.1066, 0.0818, 0.1182, 0.0811, 0.0951, 0.1265,\n",
      "           0.1061, 0.0787],\n",
      "          [0.1117, 0.0928, 0.1022, 0.0923, 0.1327, 0.1006, 0.0930, 0.1019,\n",
      "           0.0969, 0.0760]],\n",
      "\n",
      "         [[0.1000, 0.0944, 0.1097, 0.0896, 0.1121, 0.1057, 0.1044, 0.0901,\n",
      "           0.1078, 0.0862],\n",
      "          [0.0894, 0.0979, 0.1028, 0.0832, 0.0952, 0.0880, 0.1465, 0.0969,\n",
      "           0.0759, 0.1240],\n",
      "          [0.1194, 0.0779, 0.0878, 0.0942, 0.1147, 0.0882, 0.0958, 0.1023,\n",
      "           0.1052, 0.1145]],\n",
      "\n",
      "         [[0.1345, 0.0892, 0.1096, 0.1160, 0.0902, 0.0905, 0.0913, 0.1020,\n",
      "           0.0946, 0.0820],\n",
      "          [0.1061, 0.1128, 0.1198, 0.0910, 0.1007, 0.0791, 0.1070, 0.0788,\n",
      "           0.0922, 0.1126],\n",
      "          [0.1262, 0.1304, 0.0699, 0.1254, 0.0914, 0.0921, 0.0933, 0.0797,\n",
      "           0.0871, 0.1045]]]], grad_fn=<SoftmaxBackward0>)\n",
      "Value Output shape: torch.Size([1, 1])\n",
      "Value Output: tensor([[0.0605]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 运行模型\n",
    "policy_output, value_output = agent(data_tensor)\n",
    "print(\"Policy Output shape:\", policy_output.shape)\n",
    "print(\"Policy Output:\", policy_output)\n",
    "print(\"Value Output shape:\", value_output.shape)\n",
    "print(\"Value Output:\", value_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 设置量子电路数据的参数\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_timesteps = 10\n",
    "activation_probability = 0.1  # 设定任一门在任一位置激活的概率\n",
    "\n",
    "# 初始化四维数组，最后一维表示是否激活\n",
    "quantum_circuit_data = np.zeros((num_qubits, num_gate_types, num_timesteps, 2))\n",
    "\n",
    "# 随机选择一些位置设置为1，模拟量子门的激活\n",
    "np.random.seed(42)  # 设置随机种子以确保可复现性\n",
    "for qubit in range(num_qubits):\n",
    "    for gate_type in range(num_gate_types):\n",
    "        for time in range(num_timesteps):\n",
    "            if np.random.random() < activation_probability:\n",
    "                quantum_circuit_data[qubit, gate_type, time, 1] = 1  # 设置为1表示激活\n",
    "\n",
    "# 最后的一维是0和1，这里我们用第一个维度设置其它为0表示无激活\n",
    "quantum_circuit_data[:, :, :, 0] = 1 - quantum_circuit_data[:, :, :, 1]\n",
    "\n",
    "# 添加batch和channel维度\n",
    "data_tensor = torch.tensor(quantum_circuit_data, dtype=torch.float).unsqueeze(0).permute(0, 4, 3, 1, 2)\n",
    "# 这里用 permute 将数据调整为 (batch_size, channels, depth, height, width)\n",
    "# 对应的shape是 (1, 2, num_timesteps, num_gate_types, num_qubits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_gate_types = num_gate_types\n",
    "        self.num_transform_rules = num_transform_rules\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # 3D卷积层\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=3, padding=1)  # 输入通道2，输出通道16，核大小3x3x3，填充1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 池化窗口2x2x2\n",
    "\n",
    "        # 计算池化后的维度\n",
    "        pooled_dim = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "        self.fc1 = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平操作，为全连接层准备\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, self.num_qubits, self.num_gate_types, self.num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 5, 3, 10])\n",
      "Policy Output: tensor([[[[0.0965, 0.1002, 0.0988, 0.0847, 0.0906, 0.1257, 0.1106, 0.0992,\n",
      "           0.1051, 0.0887],\n",
      "          [0.0986, 0.0923, 0.1119, 0.1133, 0.0832, 0.0842, 0.0942, 0.0983,\n",
      "           0.1087, 0.1153],\n",
      "          [0.0798, 0.0791, 0.1135, 0.0975, 0.1294, 0.1040, 0.0893, 0.0996,\n",
      "           0.1098, 0.0982]],\n",
      "\n",
      "         [[0.0839, 0.1187, 0.0987, 0.1165, 0.0948, 0.0904, 0.1067, 0.0972,\n",
      "           0.1063, 0.0868],\n",
      "          [0.0818, 0.1040, 0.1112, 0.1035, 0.0896, 0.1042, 0.0954, 0.0980,\n",
      "           0.1057, 0.1067],\n",
      "          [0.1023, 0.0915, 0.0888, 0.1134, 0.0938, 0.0885, 0.1039, 0.1103,\n",
      "           0.1111, 0.0964]],\n",
      "\n",
      "         [[0.1034, 0.1086, 0.1183, 0.1005, 0.0836, 0.0905, 0.1020, 0.1053,\n",
      "           0.0872, 0.1006],\n",
      "          [0.0979, 0.0913, 0.0994, 0.0972, 0.1147, 0.0958, 0.0982, 0.0906,\n",
      "           0.1103, 0.1048],\n",
      "          [0.1128, 0.1277, 0.0836, 0.0935, 0.0945, 0.0795, 0.1063, 0.1188,\n",
      "           0.0915, 0.0917]],\n",
      "\n",
      "         [[0.0913, 0.0823, 0.1259, 0.0890, 0.1043, 0.1024, 0.0927, 0.0919,\n",
      "           0.1236, 0.0967],\n",
      "          [0.0948, 0.0824, 0.0859, 0.1015, 0.1018, 0.1090, 0.1163, 0.0892,\n",
      "           0.1112, 0.1080],\n",
      "          [0.0937, 0.1006, 0.1057, 0.0827, 0.0986, 0.0820, 0.1243, 0.1106,\n",
      "           0.0887, 0.1131]],\n",
      "\n",
      "         [[0.1105, 0.1060, 0.1019, 0.0884, 0.0974, 0.0952, 0.0807, 0.1060,\n",
      "           0.0965, 0.1175],\n",
      "          [0.1066, 0.0930, 0.1014, 0.1038, 0.0882, 0.0963, 0.1090, 0.0915,\n",
      "           0.1092, 0.1011],\n",
      "          [0.0982, 0.0976, 0.1100, 0.1118, 0.1060, 0.0906, 0.1051, 0.0969,\n",
      "           0.0904, 0.0933]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 10\n",
    "num_timesteps = 10\n",
    "\n",
    "# 实例化策略网络\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n",
    "\n",
    "# 运行模型\n",
    "output = policy_net(data_tensor)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Policy Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确保`num_qubits`, `num_gate_types`, `num_transform_rules`等参数与之前的数据结构一一对应非常重要。以下是一个更详细的代码说明，它们是如何在整个过程中保持一致的：\n",
    "\n",
    "### 数据生成与准备\n",
    "\n",
    "首先，我们要确保生成的量子电路数据与模型所期望的输入格式相匹配。这个过程涉及初始化一个四维数组来表示量子比特、量子门类型、时间步数，以及激活状态。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 设置量子电路数据的参数\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_timesteps = 10\n",
    "activation_probability = 0.1  # 设定任一门在任一位置激活的概率\n",
    "\n",
    "# 初始化四维数组，最后一维表示是否激活\n",
    "quantum_circuit_data = np.zeros((num_qubits, num_gate_types, num_timesteps, 2))\n",
    "\n",
    "# 随机选择一些位置设置为1，模拟量子门的激活\n",
    "np.random.seed(42)  # 设置随机种子以确保可复现性\n",
    "for qubit in range(num_qubits):\n",
    "    for gate_type in range(num_gate_types):\n",
    "        for time in range(num_timesteps):\n",
    "            if np.random.random() < activation_probability:\n",
    "                quantum_circuit_data[qubit, gate_type, time, 1] = 1  # 设置为1表示激活\n",
    "\n",
    "# 最后的一维是0和1，这里我们用第一个维度设置其它为0表示无激活\n",
    "quantum_circuit_data[:, :, :, 0] = 1 - quantum_circuit_data[:, :, :, 1]\n",
    "\n",
    "# 添加batch和channel维度\n",
    "data_tensor = torch.tensor(quantum_circuit_data, dtype=torch.float).unsqueeze(0).permute(0, 4, 3, 1, 2)\n",
    "# 这里用 permute 将数据调整为 (batch_size, channels, depth, height, width)\n",
    "# 对应的shape是 (1, 2, num_timesteps, num_gate_types, num_qubits)\n",
    "```\n",
    "\n",
    "### 定义3D策略网络\n",
    "\n",
    "策略网络的设计要匹配数据的维度和结构，确保输入数据能够正确处理并输出正确形状的概率分布。\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        self.num_qubits = num_qubits\n",
    "        self.num_gate_types = num_gate_types\n",
    "        self.num_transform_rules = num_transform_rules\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # 3D卷积层\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=3, padding=1)  # 输入通道2，输出通道16，核大小3x3x3，填充1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 池化窗口2x2x2\n",
    "\n",
    "        # 计算池化后的维度\n",
    "        pooled_dim = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "        self.fc1 = nn.Linear(16 * pooled_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平操作，为全连接层准备\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, self.num_qubits, self.num_gate_types, self.num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)\n",
    "```\n",
    "\n",
    "### 实例化模型并测试\n",
    "\n",
    "确保在实例化模型和准备输入数据时，所有的参数都一致。\n",
    "\n",
    "```python\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 10\n",
    "num_timesteps = 10\n",
    "\n",
    "# 实例化策略网络\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n",
    "\n",
    "# 运行模型\n",
    "output = policy_net(data_tensor)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Policy Output:\", output)\n",
    "```\n",
    "\n",
    "### 解释和验证\n",
    "\n",
    "1. **数据生成**：\n",
    "   - 我们生成了一个四维数组`quantum_circuit_data`，其形状为 `(num_qubits, num_gate_types, num_timesteps, 2)`。\n",
    "   - 最后一维表示是否激活的状态。\n",
    "   - 通过`unsqueeze(0)`添加批次维度，通过`permute`将数据调整为 `(batch_size, channels, depth, height, width)`。\n",
    "\n",
    "2. **模型设计**：\n",
    "   - 卷积层`conv1`处理输入数据，将其转换为16个通道的特征图。\n",
    "   - 池化层`pool`降低维度，提高计算效率。\n",
    "   - 全连接层`fc1`和`fc2`用于将卷积后的特征映射到策略空间（变换规则的概率分布）。\n",
    "\n",
    "3. **一致性保证**：\n",
    "   - 在数据生成、模型实例化和前向传播过程中，所有参数（如`num_qubits`, `num_gate_types`, `num_transform_rules`, `num_timesteps`）保持一致，确保数据和模型的维度匹配。\n",
    "\n",
    "通过这种方式，可以确保模型正确处理和理解量子电路数据，并输出相应的操作策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个输出是策略网络（Policy Network）生成的概率分布张量，它表示在每个量子比特、量子门类别和变换规则下采取不同操作的概率。让我们逐层解释这个张量的结构和含义。\n",
    "\n",
    "### 张量的结构和含义\n",
    "\n",
    "1. **总体结构**:\n",
    "   - 输出的张量形状是 `[1, num_qubits, num_gate_types, num_transform_rules]`，表示一个批次中的所有数据的概率分布。\n",
    "   - 在具体的示例中，`num_qubits = 5`，`num_gate_types = 3`，`num_transform_rules = 10`。\n",
    "\n",
    "2. **具体解释**:\n",
    "   - `1`: 表示批次大小，这里是1，表示我们处理一个样本。\n",
    "   - `num_qubits = 5`: 表示量子比特的数量。在这个张量中有5个切片，每个切片对应一个量子比特。\n",
    "   - `num_gate_types = 3`: 表示量子门的类别数量。每个量子比特下有3个切片，每个切片对应一个量子门类别。\n",
    "   - `num_transform_rules = 10`: 表示变换规则的数量。在每个量子比特和量子门类别的组合下，有10个值，每个值对应一个变换规则的概率。\n",
    "\n",
    "### 示例输出的具体解释\n",
    "\n",
    "考虑输出中的一个示例切片：\n",
    "\n",
    "```plaintext\n",
    "Policy Output: tensor([[[[0.0965, 0.1002, 0.0988, 0.0847, 0.0906, 0.1257, 0.1106, 0.0992, 0.1051, 0.0887],\n",
    "                        [0.0986, 0.0923, 0.1119, 0.1133, 0.0832, 0.0842, 0.0942, 0.0983, 0.1087, 0.1153],\n",
    "                        [0.0798, 0.0791, 0.1135, 0.0975, 0.1294, 0.1040, 0.0893, 0.0996, 0.1098, 0.0982]],\n",
    "\n",
    "                       [[0.0839, 0.1187, 0.0987, 0.1165, 0.0948, 0.0904, 0.1067, 0.0972, 0.1063, 0.0868],\n",
    "                        [0.0818, 0.1040, 0.1112, 0.1035, 0.0896, 0.1042, 0.0954, 0.0980, 0.1057, 0.1067],\n",
    "                        [0.1023, 0.0915, 0.0888, 0.1134, 0.0938, 0.0885, 0.1039, 0.1103, 0.1111, 0.0964]],\n",
    "\n",
    "                       [[0.1034, 0.1086, 0.1183, 0.1005, 0.0836, 0.0905, 0.1020, 0.1053, 0.0872, 0.1006],\n",
    "                        [0.0979, 0.0913, 0.0994, 0.0972, 0.1147, 0.0958, 0.0982, 0.0906, 0.1103, 0.1048],\n",
    "                        [0.1128, 0.1277, 0.0836, 0.0935, 0.0945, 0.0795, 0.1063, 0.1188, 0.0915, 0.0917]],\n",
    "\n",
    "                       [[0.0913, 0.0823, 0.1259, 0.0890, 0.1043, 0.1024, 0.0927, 0.0919, 0.1236, 0.0967],\n",
    "                        [0.0948, 0.0824, 0.0859, 0.1015, 0.1018, 0.1090, 0.1163, 0.0892, 0.1112, 0.1080],\n",
    "                        [0.0937, 0.1006, 0.1057, 0.0827, 0.0986, 0.0820, 0.1243, 0.1106, 0.0887, 0.1131]],\n",
    "\n",
    "                       [[0.1105, 0.1060, 0.1019, 0.0884, 0.0974, 0.0952, 0.0807, 0.1060, 0.0965, 0.1175],\n",
    "                        [0.1066, 0.0930, 0.1014, 0.1038, 0.0882, 0.0963, 0.1090, 0.0915, 0.1092, 0.1011],\n",
    "                        [0.0982, 0.0976, 0.1100, 0.1118, 0.1060, 0.0906, 0.1051, 0.0969, 0.0904, 0.0933]]]])\n",
    "```\n",
    "\n",
    "- **最外层列表**：表示一个批次的数据（批次大小为1）。\n",
    "- **第二层列表**：表示每个量子比特的数据（这里有5个量子比特）。\n",
    "- **第三层列表**：表示每个量子比特上的量子门类别的数据（这里每个量子比特有3种量子门类别）。\n",
    "- **第四层列表**：表示每个量子门类别下的变换规则的概率分布（这里每个量子门类别有10种变换规则）。\n",
    "\n",
    "### 例子详解\n",
    "\n",
    "```plaintext\n",
    "[[[0.0965, 0.1002, 0.0988, 0.0847, 0.0906, 0.1257, 0.1106, 0.0992, 0.1051, 0.0887],\n",
    "  [0.0986, 0.0923, 0.1119, 0.1133, 0.0832, 0.0842, 0.0942, 0.0983, 0.1087, 0.1153],\n",
    "  [0.0798, 0.0791, 0.1135, 0.0975, 0.1294, 0.1040, 0.0893, 0.0996, 0.1098, 0.0982]],\n",
    "\n",
    " [[0.0839, 0.1187, 0.0987, 0.1165, 0.0948, 0.0904, 0.1067, 0.0972, 0.1063, 0.0868],\n",
    "  [0.0818, 0.1040, 0.1112, 0.1035, 0.0896, 0.1042, 0.0954, 0.0980, 0.1057, 0.1067],\n",
    "  [0.1023, 0.0915, 0.0888, 0.1134, 0.0938, 0.0885, 0.1039, 0.1103, 0.1111, 0.0964]],\n",
    "\n",
    " [[0.1034, 0.1086, 0.1183, 0.1005, 0.0836, 0.0905, 0.1020, 0.1053, 0.0872, 0.1006],\n",
    "  [0.0979, 0.0913, 0.0994, 0.0972, 0.1147, 0.0958, 0.0982, 0.0906, 0.1103, 0.1048],\n",
    "  [0.1128, 0.1277, 0.0836, 0.0935, 0.0945, 0.0795, 0.1063, 0.1188, 0.0915, 0.0917]],\n",
    "\n",
    " [[0.0913, 0.0823, 0.1259, 0.0890, 0.1043, 0.1024, 0.0927, 0.0919, 0.1236, 0.0967],\n",
    "  [0.0948, 0.0824, 0.0859, 0.1015, 0.1018, 0.1090, 0.1163, 0.0892, 0.1112, 0.1080\n",
    "\n",
    "],\n",
    "  [0.0937, 0.1006, 0.1057, 0.0827, 0.0986, 0.0820, 0.1243, 0.1106, 0.0887, 0.1131]],\n",
    "\n",
    " [[0.1105, 0.1060, 0.1019, 0.0884, 0.0974, 0.0952, 0.0807, 0.1060, 0.0965, 0.1175],\n",
    "  [0.1066, 0.0930, 0.1014, 0.1038, 0.0882, 0.0963, 0.1090, 0.0915, 0.1092, 0.1011],\n",
    "  [0.0982, 0.0976, 0.1100, 0.1118, 0.1060, 0.0906, 0.1051, 0.0969, 0.0904, 0.0933]]]]\n",
    "```\n",
    "\n",
    "在这个示例中，每个数字表示在该量子比特、该量子门类别和该变换规则下的概率。例如，`0.0965` 表示在量子比特0，量子门类别0时，选择变换规则0的概率为9.65%。\n",
    "\n",
    "希望这些解释可以帮助您理解这个张量的结构和意义。如果有进一步的问题，请随时告诉我！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        # 输入的channel数为2，因为数据最后一维有两个通道（激活与未激活）\n",
    "        self.conv1 = nn.Conv3d(2, 16, kernel_size=(3, 3, 3), padding=1)  # 使用3x3x3的核和padding=1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)  # 使用2x2x2的池化窗口\n",
    "        # 计算池化后的维度\n",
    "        pool_size = (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2)\n",
    "        self.fc1 = nn.Linear(16 * pool_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # 展平操作，为全连接层准备\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, num_qubits, num_gate_types, num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化模型\n",
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_transform_rules = 10\n",
    "num_timesteps = 10\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "data = np.random.rand(1, 2, num_qubits, num_gate_types, num_timesteps)  # 模拟一个batch的数据\n",
    "data_tensor = torch.tensor(data, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 5, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "# 运行模型\n",
    "output = policy_net(data_tensor)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_qubits = 5\n",
    "num_gate_types = 3\n",
    "num_timesteps = 10\n",
    "num_transform_rules = 10\n",
    "batch_size = 32  # 示例批次大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PolicyNetwork3D(nn.Module):\n",
    "    def __init__(self, num_qubits, num_gate_types, num_transform_rules, num_timesteps, batch_size):\n",
    "        super(PolicyNetwork3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        # 计算池化后的尺寸，这是必须的以确保全连接层的输入尺寸正确\n",
    "        pool_size = num_timesteps // 2 * num_qubits // 2 * num_gate_types // 2\n",
    "        self.fc1 = nn.Linear(16 * pool_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_qubits * num_gate_types * num_transform_rules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * (num_timesteps // 2) * (num_qubits // 2) * (num_gate_types // 2))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, num_qubits, num_gate_types, num_transform_rules)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "policy_net = PolicyNetwork3D(num_qubits, num_gate_types, num_transform_rules, num_timesteps, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
