{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若要将一个形状为 `(max_steps, 2)` 的二维张量 `actions`，其中包含了规则索引和量子位时刻索引，转换为一个一维张量以用于类似于 `Categorical` 分布的情况，你首先需要决定如何将两个索引合并成一个单一的动作索引。这通常涉及到一种编码策略，其中将两个独立的索引合并成一个唯一的标识符。\n",
    "\n",
    "### 方案 1：线性组合\n",
    "一种常见的方法是使用线性组合将两个索引组合成一个。如果你知道第二个索引的最大值（比如 `max_qubit_moment_index`），你可以使用以下公式来生成一个唯一的动作索引：\n",
    "\n",
    "```python\n",
    "actions = actions[:, 0] * (max_qubit_moment_index + 1) + actions[:, 1]\n",
    "```\n",
    "\n",
    "这里，`actions[:, 0]` 可能代表规则索引，`actions[:, 1]` 代表量子位时刻索引。`max_qubit_moment_index` 是量子位时刻索引的最大可能值。这样做的前提是你需要知道每个索引的范围，以确保每个组合都是唯一的。\n",
    "\n",
    "### 方案 2：使用哈希或元组\n",
    "如果转换为单一索引不方便或不直观，另一个选项是在内部处理时保持使用元组或其他结构，但这需要你的模型和处理逻辑能够接受非标量的动作表示。例如，你可以直接使用元组作为字典的键或在某些特定的数据结构中使用，但这通常不适用于需要索引的神经网络模型。\n",
    "\n",
    "### 方案 3：嵌入向量\n",
    "如果动作空间较大或者组合动作的概念在问题中非常重要，另一个方法是使用嵌入（embedding）层来将动作索引转换为可以被神经网络处理的连续表示（即向量）。这涉及到为每个可能的动作索引对学习一个嵌入向量：\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设你有一个很大的动作空间\n",
    "num_actions = (max_rule_index + 1) * (max_qubit_moment_index + 1)\n",
    "action_embedding = nn.Embedding(num_actions, embedding_dim)\n",
    "\n",
    "# 你需要首先将动作转换为单一索引\n",
    "action_indices = actions[:, 0] * (max_qubit_moment_index + 1) + actions[:, 1]\n",
    "\n",
    "# 然后获取嵌入向量\n",
    "action_vectors = action_embedding(action_indices)\n",
    "```\n",
    "\n",
    "这种方法允许你在不牺牲动作的语义内容的情况下，将动作输入到神经网络中。\n",
    "\n",
    "### 选择最佳方案\n",
    "选择哪种方法取决于具体的应用场景、动作空间的大小以及模型的复杂度。如果动作空间不大，第一种方法（线性组合）通常是最简单也是最直接的。对于更复杂或更大的动作空间，可能需要考虑使用嵌入向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Indices: tensor([ 23,  67, 208])\n",
      "Action Vectors: tensor([[-1.7016,  1.7869,  0.1210,  0.1077,  1.0276, -1.8030, -1.5235, -0.0095,\n",
      "          0.6909,  1.1676],\n",
      "        [ 0.3322, -1.3147, -0.1627,  1.3009, -0.2289,  0.8109,  0.9271,  0.0722,\n",
      "          1.5754, -0.1619],\n",
      "        [ 1.3484, -1.8358,  0.6345, -0.6493,  0.2074,  0.3576, -0.5468, -0.6182,\n",
      "          1.8140, -0.5673]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假定的最大索引值\n",
    "max_rule_index = 10  # 规则索引的最大值\n",
    "max_qubit_moment_index = 20  # 量子位时刻索引的最大值\n",
    "\n",
    "# 计算动作空间的总大小\n",
    "num_actions = (max_rule_index + 1) * (max_qubit_moment_index + 1)\n",
    "\n",
    "# 嵌入层的维度\n",
    "embedding_dim = 10  # 可以根据实际需要调整\n",
    "\n",
    "# 创建嵌入层\n",
    "action_embedding = nn.Embedding(num_actions, embedding_dim)\n",
    "\n",
    "# 假设你有一批动作数据\n",
    "actions = torch.tensor([[1, 2], [3, 4], [9, 19]])  # 示例数据\n",
    "\n",
    "# 将动作数组转换为单一索引\n",
    "action_indices = actions[:, 0] * (max_qubit_moment_index + 1) + actions[:, 1]\n",
    "\n",
    "# 获取动作的嵌入向量\n",
    "action_vectors = action_embedding(action_indices)\n",
    "\n",
    "print(\"Action Indices:\", action_indices)\n",
    "print(\"Action Vectors:\", action_vectors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
